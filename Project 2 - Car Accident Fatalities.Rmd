---
title: "Project 2 - Car Accident Fatalities"
author: "John, Daniel, Jing, Michael"
date: "11/26/2020"
output: html_document
---

Our process included:
1) Data Import & Cleaning
2) Data Exploration
3) Model Exploration (Full Data Set)
4) Model Selection, Formalization, and Validation (Test & Train Data Set)
5) Highlighting Unique Observations

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(faraway) #used for VIF
library(multcomp)
library(ROCR) #Used for ROC/AUC
library(car) #used for GVIF (to address categorical variables) https://cran.r-project.org/web/packages/car/car.pdf
library(leaps) #For All Regression
library(caret)  #Used for cross validation
library(boot)  #Used for cross validation
library(jtools) #Model Comparison Table
```

# 1. Data Import &Cleaning

### Import Data & Clean

- Load Data
- Drop unwanted columns
- Remove rows with "N/A" (only one exists after unwanted columns removed)


Unwanted Columns:

- X: just a number with no corresponding contextual information
- caseid: just an identifier, not contextually relevant
- weight: used in weighting of observations, not used in our model selection process
- injury severity: will assume that this is not known when presence of fatality is unknown

```{r}

# read csv file
nass <- read.csv("nassCDS.csv", header = T)
lengths(nass)

# drop column, X, caseid, weight, injSeverity
nass=subset(nass, select = -c(X,caseid, weight, injSeverity))
lengths(nass)

# check for rows containing NA
which(is.na(nass), arr.ind=TRUE)

# remove rows containing NA
nass <- nass[complete.cases(nass), ]
lengths(nass)

```

### Convert categorical variables to factors

- Ensure categorical variables are factored
- Change "Dead" from ('alive', 'dead') to (0, 1)

```{r}
#convert categorical to factor
nass$dead<-factor(nass$dead) 
nass$dvcat<-factor(nass$dvcat)
nass$airbag<-factor(nass$airbag)
nass$seatbelt<-factor(nass$seatbelt)
nass$sex<-factor(nass$sex)
nass$abcat<-factor(nass$abcat)
nass$occRole<-factor(nass$occRole)
nass$deploy<-factor(nass$deploy)
nass$abcat<-factor(nass$abcat)

nass$dead_num <- nass$dead
levels(nass$dead_num) <- c(0,1)

#remove column of dead
nass=subset(nass, select = -c(dead))
head(nass, 5)
```



# 2. Exploratory Data Analysis
[ADD CODE / ANALYSIS HERE]
**************************
**************************
**************************
[ADD CODE / ANALYSIS HERE]


# 3. Model Exploration (Full Data Set)
Note: When developing our formal model we first split the data set into a training and test set.  However, we also performed some exploratory analysis on the full data set. Given that a model using the full data set can not be validated using ROC/AUC, this is purely for exploratory purposes. This exploratory analysis is summarized below.

### Model Selection Exploration - Full Data Set
Define full and null models - used in Forward Selection, Backward Elimination, and Stepwise Regression
```{r}
fullmod <- glm(dead_num ~. , data = nass, family = "binomial" ,maxit = 100)
Nullmod <- glm(dead_num~1, data = nass, family = "binomial")
```

All Regression
```{r}
##perform all possible regressions (1st order)
allreg <- regsubsets(dead_num ~., data=nass, nbest=5)

##create a "data frame" that stores the predictors in the various models considered as well as their various criteria
best <- as.data.frame(summary(allreg)$outmat)
best$p <- as.numeric(substr(rownames(best),1,1))+1
best$r2 <- summary(allreg)$rsq
best$adjr2 <- summary(allreg)$adjr2
best$mse <- (summary(allreg)$rss)/(dim(nass)[1]-best$p)
best$cp <- summary(allreg)$cp
best$bic <- summary(allreg)$bic
best

##Sort by:
#best[order(best$r2),]
#best[order(best$adjr2),]
best[order(best$mse),]
#best[order(best$cp),]
#best[order(best$bic),]

```


Backward Elimination
```{r}
step(fullmod, scope=list(lower=Nullmod, upper=fullmod), direction="backward")
```

Stepwise Regression
```{r}
step(Nullmod, scope=list(lower=Nullmod, upper=fullmod), direction="both")
```

Forward Selection
```{r}
# forward selection
step(Nullmod, scope=list(lower=Nullmod, upper=fullmod), direction="forward")
```

### Model Comments:
Forward Selection, Backward Elimination, and Stepwise Regression each yielded the same model. 

Choose the following variables in the model we choose based upon (i) exploratory analysis, and (ii) intuition of important factors (speed of accident, seatbelts, etc. - each seem reasonable).

- dvcat (speed)
- seatbelt
- frontal
- sex
- ageOFocc
- abcat (airbag deployment)
- occRole


Exploratory Model (full data set)
```{r}
model_picked <- glm(formula = dead_num ~ dvcat + seatbelt + frontal + sex + ageOFocc + 
    abcat + occRole, family = "binomial", data = nass, maxit = 100)

summary(model_picked)
```
Wald Test on individual predictors:

Wald test: Z = B_j - 0 / se(B_j)

Normal Distribution

Note: Can just compare z-value from summary table to qnorm(0.975)

```{r}
qnorm(0.975)
```
Can remove:

-dvcat10-24 (we chose to combine with the reference class, resulting in speed <25km/hr)
-abcatunavail (we chose to not combine with reference class given different contextual interpretations)


Generalized VIF, Delta G^2
```{r}
faraway::vif(model_picked)
car::vif(model_picked)
1-pchisq(Nullmod$deviance-model_picked$deviance, 11)
```
Note:

- VIF (faraway package) not suited to logistic regression
- GVIF^(1/2*DF) - low values indicate that multicollinearity is not present
- Delta G^2 - indicates that the model is useful (relative to the intercept only model)

#### Outliers and Influential Data Points

```{r}
# Influential Points Check
n<-nrow(nass)
p<-9

# Cook's Distance
print("cooks")
COOKS<-cooks.distance(model_picked)
COOKS[COOKS>qf(0.5,p,n-p)]

# DFFITS
print("DFFITS")
DFFITS<-dffits(model_picked)
DFFITS[abs(DFFITS)>2*sqrt(p/n)]

# DFBETAS
print('DFBETAS')
DFBETAS<-dfbetas(model_picked)
DFBETAS[abs(DFBETAS)>2/sqrt(n)]

# Plotting
par(mfrow=c(1,2))

dffits.ep <- dffits(model_picked)
plot(seq(1, length(dffits.ep), 1),dffits.ep,type="l", main="DFFITS Plot",
       xlab="Index", ylab="Dffits")
abline(h=2*sqrt(p/n), col="red")
abline(h=-2*sqrt(p/n), col="red")

dfbetas.ep <- dfbetas(model_picked)
plot(seq(1, length(dfbetas.ep), 1),dfbetas.ep,type="l", main="DFBETAS Plot",
       xlab="Index", ylab="DFBETAS")
abline(h=2*sqrt(p/n), col="red")
abline(h=-2*sqrt(p/n), col="red")

cook.ep <- cooks.distance(model_picked)
plot(seq(1, length(cook.ep), 1),cook.ep,type="l", main="Cook's Distance",
       xlab="Index", ylab="Cook's Distance", ylim=c(0,1))
abline(h=qf(0.5,p,n-p), col="red")
```
Note: DFFITS, DFBETAS, and Cook's Distance yield significantly different results.


# 4. Model Selection, Formalization, and Validation (Test & Train Data Set)

Split Data into Training & Test Set
```{r}
#creating training and testing dataset
set.seed(40)
sample<-sample.int(nrow(nass), floor(.50*nrow(nass)), replace = F)
train<-nass[sample, ]
test<-nass[-sample, ]
```

Automatic Search Processes (Backward Elimination, Stepwise Regression, Forward Selection)
```{r}
#Define Full Model and Null Model
fullmod.train <- glm(dead_num ~. , data = train, family = "binomial" ,maxit = 100)
Nullmod.train <- glm(dead_num~1, data = train, family = "binomial")

# Backward Elimination
step(fullmod.train, scope=list(lower=Nullmod.train, upper=fullmod.train), direction="backward")

# Stepwise Regression
step(Nullmod.train, scope=list(lower=Nullmod.train, upper=fullmod.train), direction="both")

# Forward Selection
step(Nullmod.train, scope=list(lower=Nullmod.train, upper=fullmod.train), direction="forward")

```
Note: 


- Each of the processes results in the same model
- Automatic search processes ignore interaction terms (explored in subsequent analysis)


Fit Model to training data
Obtain Critical value for Wald Test
```{r}
model_final <- glm(formula = dead_num ~ dvcat + seatbelt + frontal + sex + ageOFocc + 
    abcat + occRole, family = "binomial", data = train, maxit = 100)

summary(model_final)

#Critical value for Wald Test
qnorm(0.975) 
```
Note: Consistent with results from model based on full data set, we can combine dvcat10-24 with the reference class. In addition, it intuitively makes sense that differentiating speeds of relatively slow accidents may add little additional predictive power in identifying fatalities.


#### Combine appropriate accident speeds 
Combine dvcat 10-24 with reference class. Rename to "1-25"

```{r}

# Full Data Set - combine categories
nass$New_dvcat=nass$dvcat
levels(nass$New_dvcat)<-c(levels(nass$New_dvcat),"1-24")
nass$New_dvcat[nass$dvcat=="1-9km/h"]<-"1-24"
nass$New_dvcat[nass$dvcat=="10-24"]<-"1-24"
nass$New_dvcat <- droplevels(nass$New_dvcat)
levels(nass$New_dvcat)
nass$New_dvcat<- relevel(nass$New_dvcat, ref = "1-24")
levels(nass$New_dvcat)

# Test Data Set - combine categories
test$New_dvcat=test$dvcat
levels(test$New_dvcat)<-c(levels(test$New_dvcat),"1-24")
test$New_dvcat[test$dvcat=="1-9km/h"]<-"1-24"
test$New_dvcat[test$dvcat=="10-24"]<-"1-24"
test$New_dvcat <- droplevels(test$New_dvcat)
levels(test$New_dvcat)
test$New_dvcat<- relevel(test$New_dvcat, ref = "1-24")
levels(test$New_dvcat)


# Train Data Set - combine categories
train$New_dvcat=train$dvcat
levels(train$New_dvcat)<-c(levels(train$New_dvcat),"1-24")
train$New_dvcat[train$dvcat=="1-9km/h"]<-"1-24"
train$New_dvcat[train$dvcat=="10-24"]<-"1-24"
train$New_dvcat <- droplevels(train$New_dvcat)
levels(train$New_dvcat)
train$New_dvcat<- relevel(train$New_dvcat, ref = "1-24")
levels(train$New_dvcat)
```

#### Refit model based upon new categorization
```{r}
model_final <- glm(formula = dead_num ~ New_dvcat + seatbelt + frontal + sex + ageOFocc + 
    abcat + occRole, family = "binomial", data = train, maxit = 100)

summary(model_final)
```

#### Explore Interaction Terms

Interaction Terms - dvcat (speed) & seatbelt
```{r}

# New_dvcat & seatbelt

#model_reduced <- glm(formula = dead_num ~ New_dvcat + seatbelt, family = "binomial", data = nass, maxit = 100)
model_final.interaction1 <- glm(formula = dead_num ~ New_dvcat + seatbelt + frontal + sex + ageOFocc + 
    abcat + occRole + New_dvcat*seatbelt, family = "binomial", data = train, maxit = 100)
summary(model_final.interaction1)

# Delta G squared
1-pchisq(model_final$deviance-model_final.interaction1$deviance, 3)

```
Note:

- The Delta G^2 test indicates that the interaction terms are significant and should be included
- Observing the coefficients, all dvcat and the seatbelt coefficient are higher, while the interaction terms are negative (more likely to survive)


Interaction Terms - dvcat (speed) & abcat (airbag)
```{r}
# New_dvcat & abcat
model_final.interaction2 <- glm(formula = dead_num ~ New_dvcat + seatbelt + frontal + sex + ageOFocc + abcat + occRole + New_dvcat * seatbelt + New_dvcat*abcat, family = "binomial", data = train, maxit = 100)

summary(model_final.interaction2)

# Delta G squared
1-pchisq(model_final.interaction1$deviance-model_final.interaction2$deviance, 6)

```
Note:

- The Delta G^2 test indicates that given the existing predictors in the model, the proposed interaction terms (dvcat & abcat or speed and airbag) are not significant and can be excluded


Interaction Terms - dvcat (speed) & frontal
```{r}
# New_dvcat & abcat
model_final.interaction3 <- glm(formula = dead_num ~ New_dvcat + seatbelt + frontal + sex + ageOFocc + abcat + occRole + New_dvcat * seatbelt + New_dvcat*frontal, family = "binomial", data = train, maxit = 100)

summary(model_final.interaction3)

# Delta G squared
1-pchisq(model_final.interaction1$deviance-model_final.interaction3$deviance, 3)

```
Note:

- The Delta G^2 test indicates that given the existing predictors in the model, the proposed interaction terms (dvcat & abcat or speed and airbag) are not significant and can be excluded


Directly compare the coefficients of the two models in a single table
```{r}
export_summs(model_final, model_final.interaction1, scale = FALSE)
```
Note: the interaction terms are negative, but original categorical variables are higher. Look at net effect, including change in intercept


Net Effect  55+
```{r}
#Looking only at: seatbelt = none, a particular speed, change in intercept
##Note: assume everything else belongs to reference classes; also coefficient for age of occupant is unchanged

#55+: w/ interaction terms
model_final.interaction1$coefficients["seatbeltnone"]
model_final.interaction1$coefficients["New_dvcat55+"]
model_final.interaction1$coefficients["New_dvcat55+:seatbeltnone"]
model_final.interaction1$coefficients["(Intercept)"]

Total_Int <- model_final.interaction1$coefficients["New_dvcat55+"] + model_final.interaction1$coefficients["seatbeltnone"]+ model_final.interaction1$coefficients["New_dvcat55+:seatbeltnone"] + model_final.interaction1$coefficients["(Intercept)"]

#55+: w/o interaction terms
model_final$coefficients["seatbeltnone"]
model_final$coefficients["New_dvcat55+"]
model_final$coefficients["(Intercept)"]

Total <- model_final$coefficients["New_dvcat55+"] + model_final$coefficients["seatbeltnone"]+ model_final$coefficients["(Intercept)"]

print("Model w/ Interacions")
Total_Int

print("Model w/o Interacions")
Total

```

Net Effect  25-39+
```{r}
#Looking only at: seatbelt = none, a particular speed, change in intercept
##Note: assume everything else belongs to reference classes; also coefficient for age of occupant is unchanged

#55+: w/ interaction terms
model_final.interaction1$coefficients["seatbeltnone"]
model_final.interaction1$coefficients["New_dvcat25-39"]
model_final.interaction1$coefficients["New_dvcat25-39:seatbeltnone"]
model_final.interaction1$coefficients["(Intercept)"]

Total_Int25 <- model_final.interaction1$coefficients["New_dvcat25-39"] + model_final.interaction1$coefficients["seatbeltnone"]+ model_final.interaction1$coefficients["New_dvcat25-39:seatbeltnone"] + model_final.interaction1$coefficients["(Intercept)"]

#55+: w/o interaction terms
model_final$coefficients["seatbeltnone"]
model_final$coefficients["New_dvcat25-39"]
model_final$coefficients["(Intercept)"]

Total25 <- model_final$coefficients["New_dvcat25-39"] + model_final$coefficients["seatbeltnone"]+ model_final$coefficients["(Intercept)"]

print("Model w/ Interacions")
Total_Int25

print("Model w/o Interacions")
Total25

```


### Outliers and Innfluential Data Points

```{r}
# Influential Points Check
n<-nrow(train)
p<-15

# Cook's Distance
print("cooks")
COOKS<-cooks.distance(model_final.interaction1)
COOKS[COOKS>qf(0.5,p,n-p)]

# DFFITS
print("DFFITS")
DFFITS<-dffits(model_final.interaction1)
DFFITS[abs(DFFITS)>2*sqrt(p/n)]

# DFBETAS
print('DFBETAS')
DFBETAS<-dfbetas(model_final.interaction1)
DFBETAS[abs(DFBETAS)>2/sqrt(n)]

# Plotting
par(mfrow=c(1,2))

dffits.ep <- dffits(model_final.interaction1)
plot(seq(1, length(dffits.ep), 1),dffits.ep,type="l", main="DFFITS Plot",
       xlab="Index", ylab="Dffits")
abline(h=2*sqrt(p/n), col="red")
abline(h=-2*sqrt(p/n), col="red")

dfbetas.ep <- dfbetas(model_final.interaction1)
plot(seq(1, length(dfbetas.ep), 1),dfbetas.ep,type="l", main="DFBETAS Plot",
       xlab="Index", ylab="DFBETAS")
abline(h=2*sqrt(p/n), col="red")
abline(h=-2*sqrt(p/n), col="red")

cook.ep <- cooks.distance(model_final.interaction1)
plot(seq(1, length(cook.ep), 1),cook.ep,type="l", main="Cook's Distance",
       xlab="Index", ylab="Cook's Distance", ylim=c(0,1))
abline(h=qf(0.5,p,n-p), col="red")
```
Note: DFFITS, DFBETAS, and Cook's Distance yield significantly different results.

Number of influential data points (as proportion of training data):
```{r}
#Cooks
length(COOKS[COOKS>qf(0.5,p,n-p)])/nrow(train)

#DFFITS
length(DFFITS[abs(DFFITS)>2*sqrt(p/n)])/nrow(train)

#DFBETAS
length(DFBETAS[abs(DFBETAS)>2/sqrt(n)])/nrow(train)

```
Note:

- Cook's Distance - 0%
- DFFITS - 9.7%
- DFBETAS - 75.1%


### Confusion Matrices

##### Confusion Matrices - Original Model (no interaction terms - for comparison)
Explore Confusion Matrices for various values of lambda
Model without interaction terms (for comparison)
```{r}
preds.noI<-predict(model_final,newdata=test, type="response")

## Confusion Matrices - Original Model (for comparison)
table(test$dead_num, preds.noI>0.01)
table(test$dead_num, preds.noI>0.05)
table(test$dead_num, preds.noI>0.1)
table(test$dead_num, preds.noI>0.25)
table(test$dead_num, preds.noI>0.5)
```

##### Confusion Matrices (final model with interaction terms)
Explore Confusion Matrices for various values of lambda
```{r}
preds<-predict(model_final.interaction1,newdata=test, type="response")

## Confusion Matrices
table(test$dead_num, preds>0.01)
table(test$dead_num, preds>0.05)
table(test$dead_num, preds>0.1)
table(test$dead_num, preds>0.25)
table(test$dead_num, preds>0.5)
```

Given use cases (first responders, insurance companies), are more concerned with correctly identifying deaths even at the cost of higher false positive rates, select lower value of lambda. However, we would like to select a cutoff value that balances false positives with sensitivity.


##### Confusion Matrices - Detailed
After identifying lamda = .05 as roughly the cutoff that struck the optimal balance, we drilled in with greater detail:
```{r}
table(test$dead_num, preds>0.03)
table(test$dead_num, preds>0.04)
table(test$dead_num, preds>0.05)
table(test$dead_num, preds>0.06)
table(test$dead_num, preds>0.07)
```
After evaluating in greater detail, we decided to use lambda = 0.05 as our cutoff.



### Model Validation

#### Model Accuracy Details
False Positive Rate (Type I Error Rate)
False Negative Rate (Type II Error Rate)
Overall Error Rate
Sensitivity
Specificity

```{r}
#Refers to model with interaction terms

lam <- 0.05 #Selected value of lambda

#Confusion Matrix
t <- table(test$dead_num, preds>lam)
t

#Identify True Negatives, True Positives, False Negatives, False Positives
n <- nrow(test) #Number observations
TN <- t[1,1] #True Negatives
FP <- t[1,2] #False Positives
FN <- t[2,1] #False Negatives
TP <- t[2,2] #True Positives

#False Positive Rate (Type I Error Rate)
FPR <- FP / (TN+FP)
FPR

#False Negative Rate (Type II Error Rate)
FNR <- FN / (FN + TP)
FNR

#Overall Error Rate
OER <- (FP+FN) / n
OER

#Sensitivity: P(y_hat = 1 | y = 1)
sensitivity <- TP / (FN + TP)
sensitivity

#Specificity: P:(y_hat = 0 | y = 0)
specificity <- TN / (TN + FP)
specificity
```


#### ROC Curve
```{r}
#ROC
rates<-prediction(preds, test$dead_num)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")

plot(roc_result, main="Model ROC Curve")
lines(x = c(0,1), y = c(0,1), col="red")
lines(x = c(FPR, FPR), y= c(-1,2), col="blue")
```

#### AUC
```{r}
##compute the AUC
aucvalues <- performance(rates, measure = "auc")
auc <- aucvalues@y.values[[1]]
auc
```
#### GVIF, Delta G^2

Generalized VIF - detect multicollinearity 

Delta G^2 - indicates if model is useful (relative to Null model)
```{r}
faraway::vif(model_final.interaction1)
car::vif(model_final.interaction1)
1-pchisq(Nullmod.train$deviance-model_final.interaction1$deviance, 13)

#Model without interaction terms - for reference
faraway::vif(model_final)
car::vif(model_final)
1-pchisq(Nullmod.train$deviance-model_final$deviance, 10)
```
GVIF^(1/2*DF) - low values for model without interaction terms indicate that multicollinearity is not present. Once interaction terms are introduced, some multicollinearity may be present, regarding seatbelt, dvcat, and the interaction term (as expected)

Delta G^2 - indicates that the model is useful (relative to the intercept only model)



# 5. Highlighting Unique Observations
Additional Information and Facts/Highlights used in write-up

#### Survival/Death Counts
```{r}
#Number of Deaths and Accident Survivors
lengths(nass)
sum(nass$dead_num == 0)
sum(nass$dead_num == 1)
```

#### Comparison of individual variables

```{r}
# dvcat (speed) vs. seatbelt 

#dvcat (speed)
model_dvcat <- glm(formula = dead_num ~ New_dvcat, family = "binomial", data = train, maxit = 100)
summary(model_dvcat)

#seatbelt
model_seatbelt <- glm(formula = dead_num ~ seatbelt, family = "binomial", data = train, maxit = 100)
summary(model_seatbelt)

#frontal
model_frontal <- glm(formula = dead_num ~ frontal, family = "binomial", data = train, maxit = 100)
summary(model_frontal)

#abcat (airbags)
model_abcat <- glm(formula = dead_num ~ abcat, family = "binomial", data = train, maxit = 100)
summary(model_abcat)

#ageOfocc
model_age <- glm(formula = dead_num ~ ageOFocc, family = "binomial", data = train, maxit = 100)
summary(model_age)
model_NULL <- glm(formula = dead_num ~ 1, family = "binomial", data = train, maxit = 100)

model_dvcat$deviance
model_seatbelt$deviance
model_frontal$deviance
model_abcat$deviance
model_age$deviance

```


```{r}
library(Epi)
rc <- ROC(form = dead_num ~ New_dvcat + seatbelt + frontal + sex + 
    ageOFocc + abcat + occRole + New_dvcat * seatbelt, data = test, plot=c("sp", "ROC") )
## optimal combination
opt <- which.max(rowSums(rc$res[, c("sens", "spec")]))
## optimal cut-off point 
rc$res$lr.eta[opt]
```

